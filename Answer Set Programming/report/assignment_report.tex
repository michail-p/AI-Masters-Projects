% AI-2 Assignment 1 – Logic Programming
% Replace bracketed placeholders with your information before compiling.
\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{amsmath}
\setlist{noitemsep}

\begin{document}

\begin{titlepage}
    \begin{center}
        {\Large AI-2 Assignment 1 – Logic Programming}\\[1.5em]
        {\large \textbf{Name:} Michail Pettas}\\[0.5em]
        {\large \textbf{CS Username:} mai25mps}\\[0.5em]
        {\large \textbf{Course:} 5DV181}\\[0.5em]
        {\large \textbf{Assignment:} AI-2, Assignment 1 – Logic Programming}\\[0.5em]
        {\large \textbf{Date:} [30-11-25]}\\[3em]
    \end{center}
\end{titlepage}

\section{Section 1 – ASP Programs}

\subsection{Program 1 – Conflict-free Sets (\texttt{asp/conflict\_free.lp})}
\begin{itemize}
    \item \textbf{Intended rules:} \texttt{\{ in(A) \} :- arg(A).} guesses candidate extensions, while \texttt{:- in(A), in(B), att(A,B).} eliminates conflicting ones.
    \item \textbf{Guessing part:} the sole choice rule over \texttt{in/1}.
    \item \textbf{Checking part:} the integrity constraint forbidding internal attacks.
\end{itemize}

\subsection{Program 2 – Admissible Sets (\texttt{asp/admissible.lp})}
\begin{itemize}
    \item \textbf{Intended rules:} extends Program~1 with \texttt{defeated/1} plus a defence constraint so each in-argument is attacked only by arguments counter-attacked by the set.
    \item \textbf{Guessing part:} identical choice rule over \texttt{in/1}.
    \item \textbf{Checking part:} conflict-free constraint and \texttt{:- in(A), att(B,A), not defeated(B).} enforcing admissibility.
\end{itemize}

\subsection{Program 3 – Stable Extensions (\texttt{asp/stable.lp})}
\begin{itemize}
    \item \textbf{Intended rules:} builds on Program~2 and demands every outside argument is attacked by the chosen set via \texttt{:- arg(A), not in(A), not defeated(A).}
    \item \textbf{Guessing part:} choice rule over \texttt{in/1}.
    \item \textbf{Checking part:} conflict-free constraint, \texttt{defeated/1} helper, and the stability constraint.
\end{itemize}

\subsection{Program 4 – Preferred Extensions (\texttt{asp/preferred.lp})}
\begin{itemize}
    \item \textbf{Intended rules:} reuses the admissible core but incorporates a saturation-based maximality test (adapted from ASPARTIX's \texttt{prefex\_gringo.lp}). A secondary guess \texttt{inN/1} hypothesizes a strict admissible superset; if such a witness survives, the candidate is rejected.
    \item \textbf{Guessing part:} admissible choice over \texttt{in/1}.
    \item \textbf{Checking part:} admissibility constraints plus the saturation block ensuring \texttt{spoil} triggers whenever a larger admissible set exists.
    \item \textbf{Execution note:} run \texttt{clingo asp/preferred.lp <framework>.lp} to enumerate preferred extensions without Lua post-processing.
\end{itemize}

\section{Section 2 – ASP vs. Imperative Programming}

\subsection{Imperative Algorithm (Program 5 – \texttt{python/preferred.py})}
\begin{itemize}
    \item \textbf{High-level description:} performs a conflict-free backtracking search, validates admissibility at leaves, then filters inclusion-maximal sets to obtain preferred extensions.
    \item \textbf{Key steps:}
    \begin{enumerate}
        \item Sort arguments by (out+in) degree to guide branching.
        \item Recursively include/exclude arguments while preserving conflict freedom.
        \item Check admissibility after assigning all arguments.
        \item Filter maximal sets.
    \end{enumerate}
    \item \textbf{Complexity:} worst-case \(O(2^n (n+m))\) with \(n = |AR|\) and \(m = |\text{attacks}|\); ordering heuristics provide modest pruning in sparse graphs.
\end{itemize}

\subsection{Performance Comparison}
\begin{itemize}
    \item \textbf{Environment:} Windows 11 Pro, Intel i7-1185G7 @ 3.0~GHz, 16~GB RAM, Clingo~5.4.0, CPython~3.13.9.
    \item \textbf{Method:} \texttt{python/benchmark.py --runs 5 --instances examples/example1.lp ... example6.lp} for lecture examples and \texttt{--runs 3} for random frameworks. The helper runs both \texttt{clingo asp/preferred.lp} and \texttt{python/preferred.py --benchmark} on each instance and averages wall-clock time.
    \item \textbf{Notes:} timings include solver start-up; random instances come from \texttt{examples/} for reproducibility.
    \item \textbf{Timeout handling:} for \texttt{random\_n100\_*} instances, both tools are run under matching 60~s limits (Clingo via \texttt{--time-limit=60 --quiet=1}, Python via a PowerShell watchdog). Values reported as \textgreater{}60000~ms indicate the guard was intentionally hit.
\end{itemize}

\begin{longtable}{@{}lrrrrr@{}}
\toprule
Instance & \#Args & \#Attacks & ASP (ms) & Python (ms) & \#Pref \\
\midrule
example1.lp & 1 & 1 & 7.25 & 61.06 & 1 \\
example2.lp & 2 & 2 & 6.44 & 56.96 & 2 \\
example3.lp & 3 & 2 & 6.95 & 57.85 & 2 \\
example4.lp & 4 & 3 & 6.74 & 54.75 & 2 \\
example5.lp & 3 & 3 & 8.90 & 57.11 & 1 \\
example6.lp & 5 & 6 & 6.97 & 54.14 & 2 \\
random\_n10\_a05\_seed1.lp & 10 & 5 & 13.12 & 62.18 & 1 \\
random\_n10\_a10\_seed2.lp & 10 & 10 & 7.20 & 58.34 & 1 \\
random\_n10\_a20\_seed3.lp & 10 & 20 & 7.22 & 54.94 & 1 \\
random\_n20\_d025\_seed7.lp & 20 & 112 & 7.75 & 56.45 & 1 \\
random\_n100\_a10\_seed4.lp & 100 & 10 & $>$60000 & $>$60000 & -- \\
random\_n100\_a20\_seed5.lp & 100 & 20 & $>$60000 & $>$60000 & -- \\
random\_n100\_a100\_seed6.lp & 100 & 100 & $>$60000 & $>$60000 & -- \\
random\_n100\_a150\_seed7.lp & 100 & 150 & $>$60000 & $>$60000 & -- \\
\bottomrule
\end{longtable}

\subsection{Discussion}
\begin{itemize}
    \item \textbf{Relative speed:} Clingo solves each finished benchmark in \(\leq 13\)~ms, whereas Python stays between 54--62~ms principally due to interpreter start-up and a naive backtracking search.
    \item \textbf{Impact of density:} Clingo's propagation keeps timings nearly flat across 5--112 attacks. The Python implementation experiences little variation because it still explores much of the search tree.
    \item \textbf{Scalability limits:} Past roughly 20 arguments the Python solver becomes the bottleneck. For the required \texttt{random\_n100\_*} cases we deliberately enforced a strict 60~s budget on \emph{both} solvers to keep experiments manageable. Clingo therefore terminates via \texttt{--time-limit=60} after generating 0.84--1.4 million partial models, and the Python DFS is killed by a matching PowerShell watchdog. The \textgreater{}60000 entries in Table~1 explicitly signal the budget was hit rather than accidental crashes, documenting the present scalability ceiling.
    \item \textbf{Developer effort:} Declarative encodings stayed short and modular, whereas the Python solver required more parsing, bookkeeping, and benchmarking infrastructure.
    \item \textbf{Tooling insights:} \texttt{python/preferred.py --show-admissible} was useful for sanity checks, and \texttt{python/benchmark.py} guarantees reproducible comparisons under identical conditions.
\end{itemize}

\subsection{Future Work}
\begin{itemize}
    \item Add memoisation, symmetry breaking, or multiprocessing to the Python solver.
    \item Experiment with alternative ASP maximality encodings or solver configuration.
    \item Automate long-run experiments with watchdog scripts so timeouts are recorded consistently.
\end{itemize}

\section*{Appendix}
\subsection*{Timeout Command Logs}
\begin{verbatim}
$ clingo --time-limit=60 --quiet=1 asp/preferred.lp examples/random_n100_a10_seed4.lp
Answer: 1407653 ...
TIME LIMIT   : 1
Models       : 1407653+
Time         : 60.006s (Solving: 60.00s 1st Model: 0.00s Unsat: 0.00s)
\end{verbatim}

\begin{verbatim}
$ clingo --time-limit=60 --quiet=1 asp/preferred.lp examples/random_n100_a20_seed5.lp
Answer: 1341699 ...
TIME LIMIT   : 1
Models       : 1341699+
Time         : 60.011s (Solving: 60.01s 1st Model: 0.00s Unsat: 0.00s)
\end{verbatim}

\begin{verbatim}
$ clingo --time-limit=60 --quiet=1 asp/preferred.lp examples/random_n100_a100_seed6.lp
Answer: 1010729 ...
TIME LIMIT   : 1
Models       : 1010729+
Time         : 60.002s (Solving: 60.00s 1st Model: 0.00s Unsat: 0.00s)
\end{verbatim}

\begin{verbatim}
$ clingo --time-limit=60 --quiet=1 asp/preferred.lp examples/random_n100_a150_seed7.lp
Answer: 841655 ...
TIME LIMIT   : 1
Models       : 841655+
Time         : 60.006s (Solving: 60.00s 1st Model: 0.00s Unsat: 0.00s)
\end{verbatim}

\begin{verbatim}
$ pwsh -Command "cd c:/Users/vavyl/Desktop/Methods; $job = Start-Job {... python/preferred.py --input examples/random_n100_a10_seed4.lp --benchmark ...}; if (-not (Wait-Job $job -Timeout 60)) { Stop-Job $job; Write-Host 'Python solver timed out after 60s on random_n100_a10_seed4.lp'; Receive-Job $job }"
Python solver timed out after 60s on random_n100_a10_seed4.lp
\end{verbatim}

(Repeat analogous logs for \texttt{random\_n100\_a20\_seed5.lp}, \texttt{random\_n100\_a100\_seed6.lp}, and \texttt{random\_n100\_a150\_seed7.lp}.)

\end{document}
